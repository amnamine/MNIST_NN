# -*- coding: utf-8 -*-
"""MNIST using NN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/mnist-using-nn-d479b22c-1a54-4540-8bdd-d7a8f4002721.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241219/auto/storage/goog4_request%26X-Goog-Date%3D20241219T230614Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D18c98032a8e6ea8d657f013977d34b65488319b725558a234efaf784c45e4146eeadfd9c36d861fb73f1ebd8b245e6b84e8ef8a057d1a2d1565cd583d22ee6b086ff726f2c0c08b94f7688b874c2106035ed1a93db5c8a57de3c9fd3abaf15644a536a2a2bde1095d4961890c5aa2356005fa38d679143da52270d44fab74ce23fb9178fc9714823d5c6e6a59378ccee001c7a90aea6ec8e15747fd8cf93b5f94c292f354f60d675d693d6c6b928e57186473238ed9a46c780952d5ed01f3747806ebecbd05d5f05da07bd3d7662b16a6628ac2ecf945d4def6f81bf29dd6f21e405887b908178fb6070efb8bf1a72e7166efde542350c1edb1634a27d88e8a7
"""

import torch #Pytorch lib
import torch.nn as nn #neural network
import torch.optim as optim #optimazation function
from torch.utils.data import DataLoader  #data loader
import torchvision #Pytorch lib for computer vision
import torchvision.transforms as transforms #functins for transforming the dara
import matplotlib.pyplot as plt #to visualize the data

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)  # First layer: 28x28 inputs, 128 outputs
        self.fc2 = nn.Linear(128, 10)     # Second layer: 128 inputs, 10 outputs (for 10 classes)
        self.relu = nn.ReLU()             # ReLU activation function

    def forward(self, x):
        x = x.view(-1, 28*28)             # Flatten the 28x28 image to a 1D vector
        x = self.relu(self.fc1(x))        # Apply ReLU after the first layer
        x = self.fc2(x)                   # Output layer (10 values for 10 classes)
        return x

# Transformation for the dataset (Convert images to tensor and normalize)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # Normalize the images to have a mean of 0.5 and std of 0.5
])

# Load MNIST dataset
trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# DataLoader for batching, shuffling, and loading data
trainloader = DataLoader(trainset, batch_size=64, shuffle=True)
testloader = DataLoader(testset, batch_size=64, shuffle=False)

# Initialize the neural network model
model = SimpleNN()

# Define the Cross-Entropy Loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer (SGD in this case)
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Train the model
epochs = 5  # Number of training epochs
for epoch in range(epochs):
    running_loss = 0.0  # To accumulate the loss during each epoch
    for inputs, labels in trainloader:
        optimizer.zero_grad()  # Clear previous gradients

        # Forward pass: Get predictions
        outputs = model(inputs)

        # Compute the loss (how far off the predictions are from the true labels)
        loss = criterion(outputs, labels)

        # Backward pass: Calculate the gradients (how to adjust the weights)
        loss.backward()

        # Update the model's weights using the optimizer
        optimizer.step()

        # Accumulate the loss for this batch
        running_loss += loss.item()

    # Print the average loss for this epoch
    print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader)}")

# Evaluate the model's performance on the test set
correct = 0
total = 0

with torch.no_grad():  # No need to compute gradients during evaluation
    for inputs, labels in testloader:
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)  # Get the predicted class
        total += labels.size(0)  # Total number of samples
        correct += (predicted == labels).sum().item()  # Count correct predictions

accuracy = 100 * correct / total
print(f"Accuracy on the test set: {accuracy}%")